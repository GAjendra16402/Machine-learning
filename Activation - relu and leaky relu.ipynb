{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9825f28e",
   "metadata": {},
   "source": [
    "# 3. ReLU(rectified Linear Unnit)\n",
    "The rectified Linear unit (ReLu) activation function is a popular choice in deep learning neural networks due to its simplicity and effectiveness. It introduce no-linearity\n",
    "to the network, allowing it to learn and approximate complex relationship between inputs and outputs. RelU has become a standard activation function in many deep learning architectures.\n",
    "\n",
    "The ReLU function is define as follows:\n",
    "    ReLU(x) = max(0,X)\n",
    "In other words, ReLU Takes an input value x and returns the maximum of 0 and x. If x is greater than 0, ReLU outputs x directly.\n",
    "If x is less than or equal to 0, ReLU outputs 0. Therefore, the function \"rectifies\" negative values to 0 , while leaving positive values unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c481c09",
   "metadata": {},
   "source": [
    "# . ReLU has sevral desirable properties that make it an attractive choice:\n",
    "\n",
    "* Simplicity: ReLU is a simple mathematical function with low computational complexity. It only involves a single comparison and a maximum operartion.\n",
    "* Non-linearity: ReLU introduces non-linear behavior to the networrk, which allows it to learn and represent complex pattern in the data. This non-linearty is crucial for modeling highly non-linear relationship between inputs and outputs.\n",
    "* Sparse-activation: ReLU activation are sparse, meaning that only a subset of the neurons in a layer will be activated at any \n",
    "\n",
    "3. sparse activation : ReLu acitivation are spares, meaning that  only a subset of the neurons in a layer will be acitvated at any given time. This sparsity can lead to more effecient and expressive representatons, as it encourages the network to focous on the most relevant features.\n",
    "\n",
    "4. avoiding the vanishing gradient problem: ReLu helps mitigate the vanishing gradient problem, which can occur when training deep neaural networks. The vanishing gradient problem regers to the issue of gradients diminshing exponetially as they are backpropagated through many layers. Since ReLu does not saturate in the +ve range(i.e, gradients is 1 for positive inputs), if allows gradients to flow more freely and prevenst then from vanishing.\n",
    "\n",
    "5. efficient to evalute the ReLu function and its derivative.\n",
    "\n",
    "Disadvantages of ReLU:\n",
    "(1), Dead neurons: During training, some neurons may become \"dead\" or \"dying\" as they never activate (output zero) for any input. This happens when the neuron's bias term is initialized in such a way that the weighted sum of inputs is always negative. Once a neuron becomes dead, it cannot recover because the gradient of the ReLU function is zero for negative inputs. Dead neurons can lead to a decrease in the model's representational capacity.\n",
    "(2). Output saturation: ReLU saturates at zero for negative inputs. This means that when the input is negative, the gradient becomes zero, causing the neuron to be non-responsive to further changes. This saturation behavior can limit the ability of the model to learn effectively, especially in cases where negative inputs are relevant for the task.\n",
    "(3). Lack of negative output: ReLU only allows positive values or zero as output, which can be a disadvantage for certain tasks. Some data distributions or problem domains may benefit from having negative values in the output space. For example, in image generation tasks, negative pixel values can represent dark regions.\n",
    "(4). Gradient explosion: Although ReLU mitigates the vanishing gradient problem, it can still suffer from the opposite issue of gradient explosion, especially when used in deep neural networks. If the learning rate is not properly adjusted, large positive gradients can propagate through the network, causing instability and making it difficult to converge to an optimal solution.\n",
    "Adityal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a991f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# define the neural network architecture \n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "output_size = 2\n",
    "\n",
    "# define the model \n",
    "model= keras.Sequential([\n",
    "    keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "    keras.layers.Dense(output_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91ed141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "             loss = keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16533d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input and target data as Numpy arrays\n",
    "input_data = np.array([[1.0,2.0,3.0,4.0],\n",
    "                      [2.0,3.0,4.0,5.0],\n",
    "                      [3.0,4.0,5.0,6.0]])\n",
    "target_data = np.array([[0.5,0.8],\n",
    "                       [0.6,0.9],\n",
    "                       [0.7,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2da9e4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "Predicted Output: [[0.48545155 0.8061829 ]]\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "model.fit(input_data, target_data, epochs=1000, verbose=0)\n",
    "\n",
    "# test the model\n",
    "test_input = np.array([[1.0,2.0,3.0,4.0]])\n",
    "predicted_output = model.predict(test_input)\n",
    "\n",
    "print(f'Predicted Output: {predicted_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73985fe1",
   "metadata": {},
   "source": [
    "# The Leaky Rectified Linear Unit (Linear ReLU) activation function is a variation of the ReLU activation function that address some of the limitation of the standard ReLU. \n",
    "it introduces a small slope for negative values, allowing the activation function to have non-zero outputs even for negative inputs. This helps mitigate the issue of \"dying\" or \"dead\"\n",
    "neuronns in ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "534c1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(4, 8) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "# define the neural network architecture \n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "output_size = 2\n",
    "\n",
    "# create the input and target tensor\n",
    "inputs = tf.keras.Input(shape=(input_size,))\n",
    "targets = tf.keras.Input(shape=(output_size,))\n",
    "\n",
    "# define the weights and biases for the hidden layer\n",
    "hidden_weights = tf.Variable(tf.random.normal(shape=(input_size, hidden_size)))\n",
    "hidden_biases = tf.Variable(tf.zeros(shape=(hidden_size,)))\n",
    "\n",
    "# compute the hidden layer output with leaky ReLU activation function\n",
    "hidden_layer_output = tf.nn.leaky_relu(tf.matmul(inputs, hidden_weights) + hidden_biases, alpha=0.2)\n",
    "\n",
    "# define the weights and biases for the output layer\n",
    "output_weights = tf.Variable(tf.random.normal(shape=(hidden_size, output_size)))\n",
    "output_biases = tf.Variable(tf.zeros(shape=(output_size,)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4e259a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_3), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(8, 2) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "# compute the final output\n",
    "output = tf.matmul(hidden_layer_output, output_weights) + output_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb2f87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "loss = tf.reduce_mean(tf.square(output - targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c44d9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a93586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model \n",
    "model = tf.keras.Model(inputs=[inputs, targets], outputs=output)\n",
    "model.add_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bba7371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c76665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your input and target data as numpy  arrays\n",
    "input_data = np.array([[1.0,2.0,3.0,4.0]])\n",
    "target_data = np.array([[0.5,0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8dd1ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x160df32f090>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit([input_data , target_data], epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5226c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n"
     ]
    }
   ],
   "source": [
    "# test the trained network\n",
    "test_input = np.array([[1.0,2.0,3.0,4.0]])\n",
    "test_target = np.array([[0.0,0.0]])\n",
    "predicted_output = model.predict([test_input, test_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f41ef4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [[15.760935 24.332247]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicted Output: {predicted_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8bd98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

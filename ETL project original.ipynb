{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1f6a1b",
   "metadata": {},
   "source": [
    "# Web data ETL pipeline:\n",
    "    \n",
    "    a web ETL(extract, Transform, Load) pipeline is a systematic process used in data engineering to collect, transform, and load data from various sources\n",
    "    on the internet into a structured and usable format for enalysis and storage. It is essential for managinng and processing large vloumes of data of gatherd from\n",
    "    websites, online platform, and digital sources.\n",
    "    \n",
    "the process begins with data extraction, where relevant information is collected from websites, APIs, database, and other online sources.\n",
    "\n",
    "Then this raw data is tranformed through various operations, inncluding cleaning, filtering, structuring, and agregating. after transformation, the data is loaded\n",
    "into CSV file or database, making it accessible for further analysis, reporting, and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4f5aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\gajendra\n",
      "[nltk_data]     singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b687cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's start by extracting text from any article on the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddea915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "    def extract_article_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        return extract_article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c25c0",
   "metadata": {},
   "source": [
    "# In the above code, the webscraper class provides a way to conceniently extract \n",
    "the main text content of an article form a given web page URl. By creating an \n",
    "instance of the webscraper calss and calling its extract_article_text method,\n",
    "we can retrive the textual data of the article, which can then be further \n",
    "processed or analyzed as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa2f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, nltk_stopwords):\n",
    "        self.nltk_stopwords = nltk_stopwords\n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = text.split()\n",
    "        filtered_words = [words.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65fe7b",
   "metadata": {},
   "source": [
    "# In this code, the TextProcessor class provides a convenient way to process text data by tokenizing it into words and cleaning those words by removing non-alphabetic words and stopwords. It is often a crucial step in text analysis and natural language processing tasks. By creating an instance of the Text Processor class and calling its tokenize and clean method, you can obtain a list of cleaned and filtered words from a given input text. So till now, we have defined classes for scraping and preparing the data. Now we need to define a class for the entire ETL (Extract, Transform, Load) process for extracting article text, processing it, and generating a dataframe of word frequiences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96614fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.nltk_stopwords = set(stopwords.words('english'))\n",
    "    def run(self):\n",
    "        scraper = WebScraper(self.url)\n",
    "        extract_article_text = scraper.extract_article_text()\n",
    "        \n",
    "        processor = TextProcessor(self.nltk_stopwords)\n",
    "        filtered_words = processor.tokenize_and_clean(extract_article_text)\n",
    "        \n",
    "        word_freq = Counter(filtered_words)\n",
    "        df = pd.DataFrame(word_freq.items(), columns=['Words', 'Frequencies'])\n",
    "        df = df.sort_values(by='Frequencies', ascending=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe599ed",
   "metadata": {},
   "source": [
    "# In the code, the ETLPipeline class encapsulate the end-to-end process of\n",
    " extracting article text from a web page, cleaning and processing the text, calculating word frequencies, and generating a sorted Data Frame. By creating an instance of the ETL Pipeline class and calling its run method, you can perform the complete ETL process and obtain a DataFrame that provides insights into the most frequently used words in the article after removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f4843c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_article_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m article_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://groww.in/stocks/user/explore\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m ETLPipeline(article_url)\n\u001b[1;32m----> 5\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36mETLPipeline.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      6\u001b[0m     scraper \u001b[38;5;241m=\u001b[39m WebScraper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m----> 7\u001b[0m     extract_article_text \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mextract_article_text()\n\u001b[0;32m      9\u001b[0m     processor \u001b[38;5;241m=\u001b[39m TextProcessor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnltk_stopwords)\n\u001b[0;32m     10\u001b[0m     filtered_words \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenize_and_clean(extract_article_text)\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mWebScraper.extract_article_text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m html_content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m      7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extract_article_text\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_article_text' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    article_url = \"https://groww.in/stocks/user/explore\"\n",
    "    \n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e149ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\gajendra\n",
      "[nltk_data]     singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Words  Frequencies\n",
      "0  vaishnavi            1\n",
      "1       tech            1\n",
      "2      floor            1\n",
      "3   sarjapur            1\n",
      "4       main            1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def extract_article_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        article_text = ' '.join([p.text for p in soup.find_all('p')])  # Extract text from <p> tags\n",
    "        return article_text\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, nltk_stopwords):\n",
    "        self.nltk_stopwords = nltk_stopwords\n",
    "\n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
    "        return filtered_words\n",
    "\n",
    "class ETLPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        nltk.download('stopwords')\n",
    "        self.nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    def run(self):\n",
    "        scraper = WebScraper(self.url)\n",
    "        article_text = scraper.extract_article_text()\n",
    "        \n",
    "        processor = TextProcessor(self.nltk_stopwords)\n",
    "        filtered_words = processor.tokenize_and_clean(article_text)\n",
    "        \n",
    "        word_freq = Counter(filtered_words)\n",
    "        df = pd.DataFrame(word_freq.items(), columns=['Words', 'Frequencies'])\n",
    "        df = df.sort_values(by='Frequencies', ascending=False)\n",
    "        return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    article_url = \"https://groww.in/stocks/user/explore\"\n",
    "    \n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c7639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
